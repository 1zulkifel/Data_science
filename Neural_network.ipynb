{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Lists\n",
    "\n",
    "- [x] <font color='red'>Broadcasting\n",
    "- [x] Tensor dot\n",
    "- [x] Tensor reshaping\n",
    "- [x] Geometric interpretation of tensor operations\n",
    "- [x] A geometric interpretation of deep learning\n",
    "- [x] The engine of neural networks: gradient-based optimization\n",
    "- [x] Whatâ€™s a derivative?\n",
    "- [x] Derivative of a tensor operation: the gradient\n",
    "- [x] Stochastic gradient descent\n",
    "- [x] Chaining derivatives: the Backpropagation algorithm<font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WX + B\n",
    "\n",
    "1. <font color='green'> W.T(generate matrix with tranpose size of X matrix \n",
    "2. Dot_product(X,WT)\n",
    "3. After dot product add bias (b)\n",
    "4. At the end apply activation_function on each element e. sigmoid , relu softmax atc\n",
    "5. finally prdict value of Y_predict\n",
    "6. Calculte loss (Y-Y_predict)\n",
    "7. Update +W (-W),-W(+W) and b value. Backward propagation  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# relu(dot(W,input)+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3],\n",
       "       [ 4,  5,  6,  7],\n",
       "       [ 8,  9, 10, 11],\n",
       "       [12, 13, 14, 15],\n",
       "       [16, 17, 18, 19],\n",
       "       [20, 21, 22, 23]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X=np.arange(24).reshape(6,4) #input\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.43087562, -0.06590718,  0.39735761,  0.4056621 , -1.20784572,\n",
       "        -0.60734431],\n",
       "       [ 0.02065321, -0.73044686, -0.69281868,  0.79738992,  0.75192604,\n",
       "        -0.07942189],\n",
       "       [ 0.49686703,  1.03188776, -0.01075025, -0.07676662, -0.60851492,\n",
       "         2.57461277],\n",
       "       [ 0.45637699,  0.15453701,  0.54041264,  0.5135275 ,  0.19218883,\n",
       "        -0.00337617]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step 1 taking transpose\n",
    "w=np.random.randn(*X.shape)# Randomly generate weight\n",
    "w=w.T # transpose, changing rows into colums or viceversa\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-23.68924028, -24.33644215, -24.98364402, -25.63084589],\n",
       "       [ 11.54672104,  11.61400278,  11.68128451,  11.74856625],\n",
       "       [ 44.87636634,  48.28370212,  51.6910379 ,  55.09837368],\n",
       "       [ 14.11127704,  15.96494384,  17.81861064,  19.67227744]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step 2 dot product of (input,w,T)\n",
    "dot_p=np.dot(w,X)\n",
    "dot_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-22.81130787, -23.45850974, -24.1057116 , -24.75291347],\n",
       "       [ 12.42465345,  12.49193519,  12.55921693,  12.62649866],\n",
       "       [ 45.75429875,  49.16163453,  52.56897032,  55.9763061 ],\n",
       "       [ 14.98920945,  16.84287625,  18.69654306,  20.55020986]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 3 After dot product add bias (b)\n",
    "b=np.random.random(1)\n",
    "step_3=dot_p + b\n",
    "step_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [12.42465345, 12.49193519, 12.55921693, 12.62649866],\n",
       "       [45.75429875, 49.16163453, 52.56897032, 55.9763061 ],\n",
       "       [14.98920945, 16.84287625, 18.69654306, 20.55020986]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step 4 applying relu function step 3 \n",
    "#the whole proces from step_1 to step_4 is called F.p\n",
    "np.maximum(step_3,0)#relu function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] W and b are called weights or trainable perameters\n",
    "- [ ] initially these weight matrics are vary small random values ( a step is called random initializtion ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3 2.3\n"
     ]
    }
   ],
   "source": [
    "#if i cahnge x it wil cause change in y\n",
    "epsilon_x=0.3\n",
    "def f(x , epsilon_x):\n",
    "    return x+epsilon_x\n",
    "x=2\n",
    "y=f(x ,epsilon_x)\n",
    "print(x + epsilon_x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Derivative of tensor operation \\; the gradient\n",
    "# A gradient is the derivative of a tensor opration.\n",
    "#constants (x,y_pred) and w varies\n",
    "y_pred=dot(w,X)\n",
    "loss_value=loss(y_pred,y)\n",
    "loss_value=f(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#chain rule\n",
    "def a(x):\n",
    "    return x +2\n",
    "a(a(a(a(2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] calculate loss during culculating loss the value of w and b are randomly generated.\n",
    "- [ ] Now we update the weights or value of w and b in opposite to w and b\n",
    "- [ ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3],\n",
       "       [ 4,  5,  6,  7],\n",
       "       [ 8,  9, 10, 11],\n",
       "       [12, 13, 14, 15],\n",
       "       [16, 17, 18, 19],\n",
       "       [20, 21, 22, 23]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g =2 # is gradient\n",
    "step=0.2 # step size \n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03087562, -0.46590718, -0.00264239,  0.0056621 , -1.60784572,\n",
       "        -1.00734431],\n",
       "       [-0.37934679, -1.13044686, -1.09281868,  0.39738992,  0.35192604,\n",
       "        -0.47942189],\n",
       "       [ 0.09686703,  0.63188776, -0.41075025, -0.47676662, -1.00851492,\n",
       "         2.17461277],\n",
       "       [ 0.05637699, -0.24546299,  0.14041264,  0.1135275 , -0.20781117,\n",
       "        -0.40337617]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# updatindg weigts\n",
    "w1=w - step * g\n",
    "w1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stochastic gradient dencent\n",
    "- stochastic means random "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] draw a batch of training sample x and corresponding targets y\n",
    "- [x] run the network on x to obtain y_pred\n",
    "- [x] compute the loss of the network on the batch, a measure of the mismactch between y_pred and y\n",
    "- [x] compute gradient decent of the loss with regard to network's perameters (a backward pass)\n",
    "- [x] move perameters a little in the opposite direction from the gradient for example w-=step*gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Algorithm\n",
    "- [x] it is powerful algorithum for predictive analysis\n",
    "- [x] it comprises two parts \n",
    " - Naive and BIAS\n",
    " - P(H/E)=P(E/H).P(H)/P(E)\n",
    "- [x] Calulating face of in card using bias threorem example\n",
    " - p(king)= 4/52 which is 1/13\n",
    " - p(king / face)=p(face / king).p(king) / p(face)\n",
    " - p(king) =1  \n",
    " - p(face) =12 / 52 ==> 3 / 13 \n",
    " - 1(1 / 13)/(3 / 13) ==>1 / 3\n",
    " -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
